{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In computer science a sorting algorithm is an algorithm that consists of a set of instructions that takes an array as input, manipulates it using specified operations, and outputs a sorted array generally in numerical order (1). The algorithm takes the input array and outputs a permutation of the array that has been sorted. Sorting algorithms are very important for other computational algorithms to work efficiently like merge and search algorithms(2). \n",
    "\n",
    "The first sorting algorithm that was created was bubble sort in 1956 with Betty Holbertong being the pioneer of sorting algorithms in 1951. It was recognized by the early computer scientists that stable and reusable sorting algorithms would play an important role in the future of computer science(2). \n",
    "\n",
    "Sorting algorithms can be divided up into comparison sorts and integer sorts. Comparison sorts compare elements at each step of the algorithm to identify if one element should be to the left or right of another element(3).Comparison sorts are generally easier to implement than integer sorts, but comparison sorts are limited by a lower bound of O(nlogn). A comparison sort can be modeled as a large binary tree called a decision tree where each node represents a single comparison (3). At every subsequent level of the tree you divide problem into half and do constant amount of additional work. O(log N) is found when time goes up linearly while the input goes up exponentially(4). \n",
    "\n",
    "Integer sorts work differently and do not make comparisons, so they are not bounded by Ω(nlogn). Integer sorts determine for each element X how many elements are less than X and work through the array in that fashion (3). This information is used to place each element into the correct slot immediately—no need to rearrange lists.The ability to perform integer arithmetic on the keys allows integer sorting algorithms to be faster than comparison sorting algorithms(5).\n",
    "\n",
    "Sorting algorithms can be identified through their time complexity and space complexity. Time complexity is how fast the algorithm runs and is estimated by counting the number of elemenary operations performed by the algorithm, supposing that each individual operation takes a fixed amount of time to run(6). The time required by the algorithm falls under the three types: Worst case - Maximum time required by an algorithm to complete its instructions and it is most common way to analyze an algorithm. Best case - Minimum time required for the algorithm. Average case is the time required for an algorithm  and it is sometimes done while analyzing the algorithm (13). The running time of a sorting algorithm is measured in terms of big O, and not Omega and only rarely Theta. Big O notation is used to express the upper bound of an algorithm and which gives the measure for the worst time complexity for an algorithm take to complete it's job(13). For example, if an algorithm had a worst-case running time of O(nlogn), then it is understood that the algorithm will never be slower than O(nlogn), and if an algorithm has an average-case running time of O(n2)O(n2), then on average, it will not be slower than O(n2)O(n2) (7). The better the time complexity of an algorithm is, the faster the algorithm will carry out his work in practice. \n",
    "\n",
    "Space complexity is the number of memory cells which an algorithm needs to perform it's operation. A good algorithm keeps this number as small as possible. There is often a time-space-tradeoff involved in choosing the algorithm as in general problems cannot be solved with low computing time and low memory consumption (8). One then has to make a decision and to choose computing time for memory consumption or vice versa.  For example, insertion sort has a space complexity of O(1), because it doesn't need extra allocation of memory in order to sort the provided collection. In this case we say that the sorting operation is done in-place(8). In place sorting is modifying the given list by only changing the element order in the existing list. This saves on memory and is very space efficient.\n",
    "Merge sort is different, and it has a space complexity of O(n). Merge sort recursively divides the array in two  creating a new array at each step. The sorting operation requires the allocation of new space in memory. This obviously requires extra RAM and storage(9).\n",
    "\n",
    "Another important characteristic of a sorting algorithm is it's stability. Sorting stability means that records with the same key retain their relative order before and after the sort (10). Some sorting algorithms are stable by nature like Insertion sort, Merge Sort or Bubble Sort. And some sorting algorithms are not, like Heap Sort or Quick Sort. Stability is generally only important if the problem you are solving requires the rentenion of the array in the order provided. Depending on the importance of stability it is possible to choose very different sorting algorithms for the problem. If you don't need stability, you can use a fast, low space complexity algorithm like heapsort or quicksort. By contrast stable algorithms have higher big-O CPU and memory usage than unstable algorithms (11). This again goes back to the trade off between time and space complexity when choosing an appropriate sorting algorithm. \n",
    "\n",
    "The future of sorting algorithms appears to be creation hybrid algorithms like Timsort that was developed in 2002 and implemented on the Python programming language(12). Timsort is a combination of the well estabished merge sort and insertion sort. Computer scientists very early on realised the importance of these types of algorithms and it is considered that sorting algorithms are unlikely to become any more efficient than they already are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Sort\n",
    "\n",
    "Bucket sort is a comparison sort algorithm that operates on elements by dividing them into different buckets and then sorting these buckets individually. Each bucket is sorted individually using a separate sorting algorithm or by applying the bucket sort algorithm recursively. Bucket sort is mainly useful when the input is uniformly distributed over a range(14). It is a distribution sort, a generalization of pigeonhole sort, and is a cousin of radix sort in the most-to-least significant digit flavor. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity depends on the algorithm used to sort each bucket, the number of buckets to use, and whether the input is uniformly distributed. \n",
    "Bucket sort works as follows: \n",
    "Set up an array of initially empty \"buckets\".\n",
    "Scatter: Go over the original array, putting each object in its bucket.\n",
    "Sort each non-empty bucket.\n",
    "Gather: Visit the buckets in order and put all elements back into the original array(15). \n",
    "\n",
    "Negatives For the bucket sort, it’s the necessary part that the maximum value of the element must be known (16).\n",
    "\n",
    "\n",
    "Average case, best case, and worst case time complexity of this algorithm is O(n)(16). Worst-case analysis[edit]\n",
    "Bucket sort is mainly useful when input is uniformly distributed over a range. When the input contains several keys that are close to each other (clustering), those elements are likely to be placed in the same bucket, which results in some buckets containing more elements than average. The worst-case scenario occurs when all the elements are placed in a single bucket. The overall performance would then be dominated by the algorithm used to sort each bucket, which is typically \n",
    "O ( n 2 ) {\\displaystyle O(n^{2})} \n",
    " insertion sort, making bucket sort less optimal than \n",
    "O ( n log ⁡ ( n ) ) {\\displaystyle O(n\\log(n))} \n",
    " comparison sort algorithms like Quicksort. \n",
    " Optimizations\n",
    "A common optimization is to put the unsorted elements of the buckets back in the original array first, then run insertion sort over the complete array; because insertion sort's runtime is based on how far each element is from its final position, the number of comparisons remains relatively small, and the memory hierarchy is better exploited by storing the list contiguously in memory(15).  \n",
    "\n",
    "![Bucket_sort_1](http://localhost:8888/view/Bucket_sort_1.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Python3 program to sort an array  \n",
    "# using bucket sort \n",
    "import time;\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def insertionSort(b): \n",
    "    for i in range(1, len(b)): \n",
    "        up = b[i] \n",
    "        j = i - 1\n",
    "        while j >=0 and b[j] > up:  \n",
    "            b[j + 1] = b[j] \n",
    "            j -= 1\n",
    "        b[j + 1] = up     \n",
    "    return b      \n",
    "              \n",
    "def bucketSort(x): \n",
    "    arr = [] \n",
    "    slot_num = 10 # 10 means 10 slots, each \n",
    "                  # slot's size is 0.1 \n",
    "    for i in range(slot_num): \n",
    "        arr.append([]) \n",
    "          \n",
    "    # Put array elements in different buckets  \n",
    "    for j in x: \n",
    "        index_b = int(slot_num * j)  \n",
    "        arr[index_b].append(j) \n",
    "      \n",
    "    # Sort individual buckets  \n",
    "    for i in range(slot_num): \n",
    "        arr[i] = insertionSort(arr[i]) \n",
    "          \n",
    "    # concatenate the result \n",
    "    k = 0\n",
    "    for i in range(slot_num): \n",
    "        for j in range(len(arr[i])): \n",
    "            x[k] = arr[i][j] \n",
    "            k += 1\n",
    "    return x \n",
    "  \n",
    "# Driver Code \n",
    "x = [0.897, 0.565, 0.656, \n",
    "     0.1234, 0.665, 0.3434]  \n",
    "print(\"Sorted Array is\") \n",
    "print(bucketSort(x)) \n",
    "  \n",
    "# This code is contributed by \n",
    "# Oneil Hsiao \n",
    "\n",
    "end_time = time.time()\n",
    "time_elapsed = end_time - start_time \n",
    "print(\"Benchmark time equals\", time_elapsed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Sort \n",
    "\n",
    "The selection sort improves on the bubble sort by making only one exchange for every pass through the list. In order to do this, a selection sort looks for the largest value as it makes a pass and, after completing the pass, places it in the proper location. As with a bubble sort, after the first pass, the largest item is in the correct place. After the second pass, the next largest is in place. This process continues and requires \n",
    "n−1\n",
    "n−1\n",
    "passes to sort n items, since the final item must be in place after the \n",
    "(n−1)\n",
    "(n−1)\n",
    "st pass. You may see that the selection sort makes the same number of comparisons as the bubble sort and is therefore also \n",
    "O(\n",
    "n\n",
    "2\n",
    ")\n",
    "O(n2)\n",
    ". However, due to the reduction in the number of exchanges, the selection sort typically executes faster in benchmark studies. In fact, for our list, the bubble sort makes 20 exchanges, while the selection sort makes only 8. (17)\n",
    "\n",
    "The selection sort algorithm sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted part and putting it at the beginning. The algorithm maintains two subarrays in a given array.\n",
    "1) The subarray which is already sorted.\n",
    "2) Remaining subarray which is unsorted.\n",
    "In every iteration of selection sort, the minimum element (considering ascending order) from the unsorted subarray is picked and moved to the sorted subarray(18). \n",
    "\n",
    "Time Complexity\n",
    "Best Case = O(n)2\n",
    "Average Case = O(n)2\n",
    "Worst Case = O(n)2\n",
    "Note: for larger arrays we never use selection sort.\n",
    "Python Selection Sort Program (19)\n",
    "\n",
    "For an unsorted sequence of length n the algorithm requires n step for the first scan then at each iteration it reduces by 1. Mathematically this can be expressed as,\n",
    "T(n) = n + (n-1) + (n-2) + ....+ 1 = n(n+1)/2 = O(n2)\n",
    "The above expression concludes that this algorithm is proportional to n2. Therefore for a given list of size n, the time complexity of selection sort can be expressed as,\n",
    "Worst Case Time Complexity [ Big-O ]: O(n2)\n",
    "Best Case Time Complexity [Big-omega]: O(n2)\n",
    "Average Time Complexity [Big-theta]: O(n2)\n",
    "Space Complexity: O(1) (20)\n",
    "\n",
    "Bubble sort selects the maximum remaining elements at each stage, but wastes some effort imparting some order to an unsorted part of the array.\n",
    "Selection sort is quadratic in both the worst and the average case, and requires no extra memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program for implementation of Selection \n",
    "# Sort \n",
    "import sys \n",
    "A = [64, 25, 12, 22, 11] \n",
    "\n",
    "# Traverse through all array elements \n",
    "for i in range(len(A)): \n",
    "\t\n",
    "\t# Find the minimum element in remaining \n",
    "\t# unsorted array \n",
    "\tmin_idx = i \n",
    "\tfor j in range(i+1, len(A)): \n",
    "\t\tif A[min_idx] > A[j]: \n",
    "\t\t\tmin_idx = j \n",
    "\t\t\t\n",
    "\t# Swap the found minimum element with \n",
    "\t# the first element\t\t \n",
    "\tA[i], A[min_idx] = A[min_idx], A[i] \n",
    "\n",
    "# Driver code to test above \n",
    "print (\"Sorted array\") \n",
    "for i in range(len(A)): \n",
    "\tprint(\"%d\" %A[i]), \n",
    "### https://www.geeksforgeeks.org/python-program-for-selection-sort/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MergeSort \n",
    "Mergesort\n",
    "Merge-sort is based on the divide-and-conquer paradigm. It involves the following three steps: \n",
    "Divide the array into two (or more) subarrays \n",
    "Sort each subarray (Conquer) \n",
    "Merge them into one (in a smart way!) (21)\n",
    "\n",
    "Complexity of Mergesort\n",
    "Suppose T(n) is the number of comparisons needed to sort an array of n elements by the MergeSort algorithm. By splitting an array in two parts we reduced a problem to sorting two parts but smaller sizes, namely n/2. Each part can be sort in T(n/2). Finally, on the last step we perform n-1 comparisons to merge these two parts in one.\n",
    "\n",
    "The first algorithm we will study is the merge sort. Merge sort is a recursive algorithm that continually splits a list in half. If the list is empty or has one item, it is sorted by definition (the base case). If the list has more than one item, we split the list and recursively invoke a merge sort on both halves. Once the two halves are sorted, the fundamental operation, called a merge, is performed. Merging is the process of taking two smaller sorted lists and combining them together into a single, sorted, new list. Figure 10 shows our familiar example list as it is being split by. (22)\n",
    "In order to analyze the mergeSort function, we need to consider the two distinct processes that make up its implementation. First, the list is split into halves. We already computed (in a binary search) that we can divide a list in half \n",
    "logn\n",
    "log⁡n\n",
    "times where n is the length of the list. The second process is the merge. Each item in the list will eventually be processed and placed on the sorted list. So the merge operation which results in a list of size n requires n operations. The result of this analysis is that \n",
    "logn\n",
    "log⁡n\n",
    "splits, each of which costs \n",
    "n\n",
    "n\n",
    "for a total of \n",
    "nlogn\n",
    "nlog⁡n\n",
    "operations. A merge sort is an \n",
    "O(nlogn)\n",
    "O(nlog⁡n)\n",
    "algorithm.\n",
    "Recall that the slicing operator is \n",
    "O(k)\n",
    "O(k)\n",
    "where k is the size of the slice. In order to guarantee that mergeSort will be \n",
    "O(nlogn)\n",
    "O(nlog⁡n)\n",
    "we will need to remove the slice operator. Again, this is possible if we simply pass the starting and ending indices along with the list when we make the recursive call. We leave this as an exercise.\n",
    "It is important to notice that the mergeSort function requires extra space to hold the two halves as they are extracted with the slicing operations. This additional space can be a critical factor if the list is large and can make this sort problematic when working on large data sets.\n",
    "\n",
    "Merge Sort is useful for sorting linked lists in O(nLogn) time.In case of linked lists the case is different mainly due to difference in memory allocation of arrays and linked lists. Unlike arrays, linked list nodes may not be adjacent in memory. Unlike array, in linked list, we can insert items in the middle in O(1) extra space and O(1) time. Therefore merge operation of merge sort can be implemented without extra space for linked lists. \n",
    "In arrays, we can do random access as elements are continuous in memory. Let us say we have an integer (4-byte) array A and let the address of A[0] be x then to access A[i], we can directly access the memory at (x + i*4). Unlike arrays, we can not do random access in linked list. Quick Sort requires a lot of this kind of access. In linked list to access i’th index, we have to travel each and every node from the head to i’th node as we don’t have continuous block of memory. Therefore, the overhead increases for quick sort. Merge sort accesses data sequentially and the need of random access is low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program for implementation of MergeSort \n",
    "def mergeSort(arr): \n",
    "\tif len(arr) >1: \n",
    "\t\tmid = len(arr)//2 #Finding the mid of the array \n",
    "\t\tL = arr[:mid] # Dividing the array elements \n",
    "\t\tR = arr[mid:] # into 2 halves \n",
    "\n",
    "\t\tmergeSort(L) # Sorting the first half \n",
    "\t\tmergeSort(R) # Sorting the second half \n",
    "\n",
    "\t\ti = j = k = 0\n",
    "\t\t\n",
    "\t\t# Copy data to temp arrays L[] and R[] \n",
    "\t\twhile i < len(L) and j < len(R): \n",
    "\t\t\tif L[i] < R[j]: \n",
    "\t\t\t\tarr[k] = L[i] \n",
    "\t\t\t\ti+=1\n",
    "\t\t\telse: \n",
    "\t\t\t\tarr[k] = R[j] \n",
    "\t\t\t\tj+=1\n",
    "\t\t\tk+=1\n",
    "\t\t\n",
    "\t\t# Checking if any element was left \n",
    "\t\twhile i < len(L): \n",
    "\t\t\tarr[k] = L[i] \n",
    "\t\t\ti+=1\n",
    "\t\t\tk+=1\n",
    "\t\t\n",
    "\t\twhile j < len(R): \n",
    "\t\t\tarr[k] = R[j] \n",
    "\t\t\tj+=1\n",
    "\t\t\tk+=1\n",
    "\n",
    "# Code to print the list \n",
    "def printList(arr): \n",
    "\tfor i in range(len(arr)):\t\t \n",
    "\t\tprint(arr[i],end=\" \") \n",
    "\tprint() \n",
    "\n",
    "# driver code to test the above code \n",
    "if __name__ == '__main__': \n",
    "\tarr = [12, 11, 13, 5, 6, 7] \n",
    "\tprint (\"Given array is\", end=\"\\n\") \n",
    "\tprintList(arr) \n",
    "\tmergeSort(arr) \n",
    "\tprint(\"Sorted array is: \", end=\"\\n\") \n",
    "\tprintList(arr) \n",
    "\n",
    "# This code is contributed by Mayank Khanna \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quicksort\n",
    "The quick sort uses divide and conquer to gain the same advantages as the merge sort, while not using additional storage. As a trade-off, however, it is possible that the list may not be divided in half. When this happens, we will see that performance is diminished.\n",
    "A quick sort first selects a value, which is called the pivot value. Although there are many different ways to choose the pivot value, we will simply use the first item in the list. The role of the pivot value is to assist with splitting the list. The actual position where the pivot value belongs in the final sorted list, commonly called the split point, will be used to divide the list for subsequent calls to the quick sort.\n",
    "Figure 12 shows that 54 will serve as our first pivot value. Since we have looked at this example a few times already, we know that 54 will eventually end up in the position currently holding 31. The partition process will happen next. It will find the split point and at the same time move other items to the appropriate side of the list, either less than or greater than the pivot value.(24) Partitioning begins by locating two position markers—let’s call them leftmark and rightmark—at the beginning and end of the remaining items in the list (positions 1 and 8 in Figure 13). The goal of the partition process is to move items that are on the wrong side with respect to the pivot value while also converging on the split point. Figure 13 shows this process as we locate the position of 54.\n",
    "\n",
    "To analyze the quickSort function, note that for a list of length n, if the partition always occurs in the middle of the list, there will again be \n",
    "logn\n",
    "log⁡n\n",
    "divisions. In order to find the split point, each of the n items needs to be checked against the pivot value. The result is \n",
    "nlogn\n",
    "nlog⁡n\n",
    ". In addition, there is no need for additional memory as in the merge sort process.\n",
    "Unfortunately, in the worst case, the split points may not be in the middle and can be very skewed to the left or the right, leaving a very uneven division. In this case, sorting a list of n items divides into sorting a list of 0 items and a list of \n",
    "n−1\n",
    "n−1\n",
    "items. Then sorting a list of \n",
    "n−1\n",
    "n−1\n",
    "divides into a list of size 0 and a list of size \n",
    "n−2\n",
    "n−2\n",
    ", and so on. The result is an \n",
    "O(\n",
    "n\n",
    "2\n",
    ")\n",
    "O(n2)\n",
    "sort with all of the overhead that recursion requires.\n",
    "We mentioned earlier that there are different ways to choose the pivot value. In particular, we can attempt to alleviate some of the potential for an uneven division by using a technique called median of three. To choose the pivot value, we will consider the first, the middle, and the last element in the list. In our example, those are 54, 77, and 20. Now pick the median value, in our case 54, and use it for the pivot value (of course, that was the pivot value we used originally). The idea is that in the case where the the first item in the list does not belong toward the middle of the list, the median of three will choose a better “middle” value. This will be particularly useful when the original list is somewhat sorted to begin with. We leave the implementation of this pivot value selection as an exercise. \n",
    "\n",
    "here are many different versions of quickSort that pick pivot in different ways. \n",
    "Always pick first element as pivot.\n",
    "Always pick last element as pivot (implemented below)\n",
    "Pick a random element as pivot.\n",
    "Pick median as pivot.\n",
    "The key process in quickSort is partition(). Target of partitions is, given an array and an element x of array as pivot, put x at its correct position in sorted array and put all smaller elements (smaller than x) before x, and put all greater elements (greater than x) after x. All this should be done in linear time(25).\n",
    "\n",
    "When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort. \n",
    "In efficient implementations it is not a stable sort, meaning that the relative order of equal sort items is not preserved. Quicksort can operate in-place on an array, requiring small additional amounts of memory to perform the sorting. It is very similar to selection sort, except that it does not always choose worst-case partition. (26) Formal analysis[edit]\n",
    "Worst-case analysis[edit]\n",
    "The most unbalanced partition occurs when one of the sublists returned by the partitioning routine is of size n − 1.[27] This may occur if the pivot happens to be the smallest or largest element in the list, or in some implementations (e.g., the Lomuto partition scheme as described above) when all the elements are equal. \n",
    "If this happens repeatedly in every partition, then each recursive call processes a list of size one less than the previous list. Consequently, we can make n − 1 nested calls before we reach a list of size 1. This means that the call tree is a linear chain of n − 1 nested calls. The ith call does O(n − i) work to do the partition, and \n",
    "∑ i = 0 n ( n − i ) = O ( n 2 ) {\\displaystyle \\textstyle \\sum _{i=0}^{n}(n-i)=O(n^{2})} \n",
    ", so in that case Quicksort takes O(n²) time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program for implementation of Quicksort Sort \n",
    "\n",
    "# This function takes last element as pivot, places \n",
    "# the pivot element at its correct position in sorted \n",
    "# array, and places all smaller (smaller than pivot) \n",
    "# to left of pivot and all greater elements to right \n",
    "# of pivot \n",
    "def partition(arr,low,high): \n",
    "\ti = ( low-1 )\t\t # index of smaller element \n",
    "\tpivot = arr[high]\t # pivot \n",
    "\n",
    "\tfor j in range(low , high): \n",
    "\n",
    "\t\t# If current element is smaller than or \n",
    "\t\t# equal to pivot \n",
    "\t\tif arr[j] <= pivot: \n",
    "\t\t\n",
    "\t\t\t# increment index of smaller element \n",
    "\t\t\ti = i+1\n",
    "\t\t\tarr[i],arr[j] = arr[j],arr[i] \n",
    "\n",
    "\tarr[i+1],arr[high] = arr[high],arr[i+1] \n",
    "\treturn ( i+1 ) \n",
    "\n",
    "# The main function that implements QuickSort \n",
    "# arr[] --> Array to be sorted, \n",
    "# low --> Starting index, \n",
    "# high --> Ending index \n",
    "\n",
    "# Function to do Quick sort \n",
    "def quickSort(arr,low,high): \n",
    "\tif low < high: \n",
    "\n",
    "\t\t# pi is partitioning index, arr[p] is now \n",
    "\t\t# at right place \n",
    "\t\tpi = partition(arr,low,high) \n",
    "\n",
    "\t\t# Separately sort elements before \n",
    "\t\t# partition and after partition \n",
    "\t\tquickSort(arr, low, pi-1) \n",
    "\t\tquickSort(arr, pi+1, high) \n",
    "\n",
    "# Driver code to test above \n",
    "arr = [10, 7, 8, 9, 1, 5] \n",
    "n = len(arr) \n",
    "quickSort(arr,0,n-1) \n",
    "print (\"Sorted array is:\") \n",
    "for i in range(n): \n",
    "\tprint (\"%d\" %arr[i]), \n",
    "\n",
    "# This code is contributed by Mohit Kumra \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timsort\n",
    "\n",
    " Tim Peters created Timsort for the Python programming language in 2001. Timsort first analyses the list it is trying to sort and then chooses an approach based on the analysis of the list.\n",
    "Since the algorithm has been invented it has been used as the default sorting algorithm in Python, Java, the Android Platform, and in GNU Octave(27). Timsort’s sorting time is the same as Mergesort, which is faster than most of the other sorts you might know. Timsort actually makes use of Insertion sort and Mergesort, as you’ll see soon.\n",
    "Peters designed Timsort to use already-ordered elements that exist in most real-world data sets. It calls these already-ordered elements “natural runs”. It iterates over the data collecting the elements into runs and simultaneously merging those runs together into one.\n",
    "\n",
    "imsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. The algorithm finds subsequences of the data that are already ordered, and uses that knowledge to sort the remainder more efficiently. This is done by merging an identified subsequence, called a run, with existing runs until certain criteria are fulfilled.(28)\n",
    "\n",
    "Timsort was designed to take advantage of runs of consecutive ordered elements that already exist in most real-world data, natural runs. It iterates over the data collecting elements into runs, and simultaneously merging those runs together. When there are runs, doing this decreases the total number of comparisons needed to fully sort the list. \n",
    "Runs[edit]\n",
    "Timsort iterates over the data looking for natural runs of at least two elements that are either non-descending (each element is greater than or equal to its predecessor) or strictly descending (each element is less than its predecessor). Since descending runs are later blindly reversed, excluding equal elements maintains the algorithm's stability; i.e., equal elements won't be reversed. Note that any two elements are guaranteed to be either descending or non-descending. \n",
    "A reference to each run is then pushed onto a stack.\n",
    "In the worst case, Timsort takes \n",
    "O ( n log ⁡ n ) {\\displaystyle O(n\\log n)} \n",
    " comparisons to sort an array of n elements. In the best case, which occurs when the input is already sorted, it runs in linear time, meaning that it is an adaptive sorting algorithm.[3] \n",
    " \n",
    " Timsort is an adaptive sort,[8] using insertion sort to combine runs smaller than the minimum run size (minrun), and merge sort otherwise. \n",
    "Minrun is selected so most runs in a random array are, or become, minrun in length. It also results in a reasonable number of function calls in the implementation of the sort.\n",
    "\n",
    "Concurrently with the search for runs, the runs are merged with mergesort. Except where Timsort tries to optimise for merging disjoint runs in galloping mode, runs are repeatedly merged two at a time, with the only concerns being to maintain stability and merge balance. \n",
    "Stability requires non-consecutive runs are not merged, as elements could be transferred across equal elements in the intervening run, violating stability. Further, it would be impossible to recover the order of the equal elements at a later point. \n",
    "\n",
    "Timsort performs an almost in-place merge sort, as actual in-place merge sort implementations have a high overhead. First Timsort performs a binary search to find the location in the first run of the first element in the second run, and the location in the second run of the last element in the first run. Elements before and after these locations are already in their correct place, and may be removed from further consideration. This not only optimises element movements and running time, but also allows the use of less temporary memory. Then the smaller of the remaining runs is copied into temporary memory, and elements are merged with the larger run, into the now free space. If the first run is smaller, the merge starts at the beginning; if the second is smaller, the merge starts at the end. \n",
    "Say, for example, two runs A and B are to be merged, with A being the smaller run. In this case a binary search examines A to find the first element aʹ larger than the first element of B. Note that A and B are already sorted individually. When aʹ is found, the algorithm can ignore elements before that position when merging. Similarly, the algorithm also looks for the first element bʹ in B greater than the last element of A. The elements after bʹ can also be ignored when merging. This preliminary searching is not efficient for highly random data, but is efficient in other situations and is hence included. \n",
    "\n",
    "In galloping mode, the algorithm searches for the first element of one array in the other. This is done by comparing that initial element with the (2k − 1)th element of the other array (first, third, seventh, and so on) so as to get a range of elements between which the initial element will lie. This shortens the range for binary searching, thus increasing efficiency. In cases where galloping is found to be less efficient than binary search, galloping mode is exited. \n",
    "\n",
    "When merging is done right-to-left, galloping starts from the right end of the data, that is, the last element. Galloping from the beginning also gives the required results, but makes more comparisons. Thus, the galloping algorithm uses a variable that gives the index at which galloping should begin. Timsort can enter galloping mode at any index and continue checking at the next index which is offset by 1, 3, 7, …, (2k − 1)… and so on from the current index. In the case of right-to-left merging, the offsets to the index will be −1, −3, −7, …[8] \n",
    "Galloping is not always efficient. In some cases galloping mode requires more comparisons than a simple linear search. While for the first few cases both modes may require the same number of comparisons, over time galloping mode requires 33% more comparisons than linear search to arrive at the same results. \n",
    "\n",
    "In short, Timsort does 2 things incredibly well:\n",
    "Great performance on arrays with preexisting internal structure\n",
    "Being able to maintain a stable sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 program to perform TimSort. \n",
    "RUN = 32\n",
    "\t\n",
    "# This function sorts array from left index to \n",
    "# to right index which is of size atmost RUN \n",
    "def insertionSort(arr, left, right): \n",
    "\n",
    "\tfor i in range(left + 1, right+1): \n",
    "\t\n",
    "\t\ttemp = arr[i] \n",
    "\t\tj = i - 1\n",
    "\t\twhile arr[j] > temp and j >= left: \n",
    "\t\t\n",
    "\t\t\tarr[j+1] = arr[j] \n",
    "\t\t\tj -= 1\n",
    "\t\t\n",
    "\t\tarr[j+1] = temp \n",
    "\t\n",
    "# merge function merges the sorted runs \n",
    "def merge(arr, l, m, r): \n",
    "\n",
    "\t# original array is broken in two parts \n",
    "\t# left and right array \n",
    "\tlen1, len2 = m - l + 1, r - m \n",
    "\tleft, right = [], [] \n",
    "\tfor i in range(0, len1): \n",
    "\t\tleft.append(arr[l + i]) \n",
    "\tfor i in range(0, len2): \n",
    "\t\tright.append(arr[m + 1 + i]) \n",
    "\t\n",
    "\ti, j, k = 0, 0, l \n",
    "\t# after comparing, we merge those two array \n",
    "\t# in larger sub array \n",
    "\twhile i < len1 and j < len2: \n",
    "\t\n",
    "\t\tif left[i] <= right[j]: \n",
    "\t\t\tarr[k] = left[i] \n",
    "\t\t\ti += 1\n",
    "\t\t\n",
    "\t\telse: \n",
    "\t\t\tarr[k] = right[j] \n",
    "\t\t\tj += 1\n",
    "\t\t\n",
    "\t\tk += 1\n",
    "\t\n",
    "\t# copy remaining elements of left, if any \n",
    "\twhile i < len1: \n",
    "\t\n",
    "\t\tarr[k] = left[i] \n",
    "\t\tk += 1\n",
    "\t\ti += 1\n",
    "\t\n",
    "\t# copy remaining element of right, if any \n",
    "\twhile j < len2: \n",
    "\t\tarr[k] = right[j] \n",
    "\t\tk += 1\n",
    "\t\tj += 1\n",
    "\t\n",
    "# iterative Timsort function to sort the \n",
    "# array[0...n-1] (similar to merge sort) \n",
    "def timSort(arr, n): \n",
    "\n",
    "\t# Sort individual subarrays of size RUN \n",
    "\tfor i in range(0, n, RUN): \n",
    "\t\tinsertionSort(arr, i, min((i+31), (n-1))) \n",
    "\t\n",
    "\t# start merging from size RUN (or 32). It will merge \n",
    "\t# to form size 64, then 128, 256 and so on .... \n",
    "\tsize = RUN \n",
    "\twhile size < n: \n",
    "\t\n",
    "\t\t# pick starting point of left sub array. We \n",
    "\t\t# are going to merge arr[left..left+size-1] \n",
    "\t\t# and arr[left+size, left+2*size-1] \n",
    "\t\t# After every merge, we increase left by 2*size \n",
    "\t\tfor left in range(0, n, 2*size): \n",
    "\t\t\n",
    "\t\t\t# find ending point of left sub array \n",
    "\t\t\t# mid+1 is starting point of right sub array \n",
    "\t\t\tmid = left + size - 1\n",
    "\t\t\tright = min((left + 2*size - 1), (n-1)) \n",
    "\t\n",
    "\t\t\t# merge sub array arr[left.....mid] & \n",
    "\t\t\t# arr[mid+1....right] \n",
    "\t\t\tmerge(arr, left, mid, right) \n",
    "\t\t\n",
    "\t\tsize = 2*size \n",
    "\t\t\n",
    "# utility function to print the Array \n",
    "def printArray(arr, n): \n",
    "\n",
    "\tfor i in range(0, n): \n",
    "\t\tprint(arr[i], end = \" \") \n",
    "\tprint() \n",
    "\n",
    "\t\n",
    "# Driver program to test above function \n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "\tarr = [5, 21, 7, 23, 19] \n",
    "\tn = len(arr) \n",
    "\tprint(\"Given Array is\") \n",
    "\tprintArray(arr, n) \n",
    "\t\n",
    "\ttimSort(arr, n) \n",
    "\t\n",
    "\tprint(\"After Sorting Array is\") \n",
    "\tprintArray(arr, n) \n",
    "\t\n",
    "# This code is contributed by Rituraj Jain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[Bucket_sort_1]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![Bucket_sort_1](http://localhost:8888/view/Bucket_sort_1.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://brilliant.org/wiki/sorting-algorithms/  Visited on the 17/4/19 \n",
    "2. https://en.wikipedia.org/wiki/Sorting_algorithm Visited on the 17/4/19\n",
    "3. https://betterexplained.com/articles/sorting-algorithms/ Visited on the 18/4/19\n",
    "4. https://stackoverflow.com/questions/2307283/what-does-olog-n-mean-exactly Visited on the 18/4/19\n",
    "5. https://en.wikipedia.org/wiki/Integer_sorting Visited on the 18/4/19\n",
    "6. https://en.wikipedia.org/wiki/Time_complexity Visited on the 18/4/19\n",
    "7. http://www.leda-tutorial.org/en/official/ch02s02s03.html Visited on the 18/4/19 \n",
    "8. https://brilliant.org/wiki/space-complexity/ Visited on the 18/4/19\n",
    "9. https://stackoverflow.com/questions/16585507/sorting-in-place \n",
    "10. https://stackoverflow.com/questions/1517793/what-is-stability-in-sorting-algorithms-and-why-is-it-importan\n",
    "11. https://brilliant.org/wiki/sorting-algorithms/\n",
    "12. https://en.wikipedia.org/wiki/Timsort\n",
    "13. https://www.datacamp.com/community/tutorials/analyzing-complexity-code-python\n",
    "14. https://www.hackerearth.com/practice/algorithms/sorting/bucket-sort/tutorial/\n",
    "15. https://en.wikipedia.org/wiki/Bucket_sort\n",
    "16. https://www.includehelp.com/algorithms/bucket-sort-algorithm.aspx \n",
    "17. http://interactivepython.org/runestone/static/pythonds/SortSearch/TheSelectionSort.html \n",
    "18. https://www.geeksforgeeks.org/python-program-for-selection-sort/ \n",
    "19. https://www.thecrazyprogrammer.com/2017/11/python-selection-sort.html\n",
    "20. https://djangocentral.com/selection-sort-in-python/\n",
    "21. https://www.cs.cmu.edu/~adamchik/15-121/lectures/Sorting%20Algorithms/sorting.html\n",
    "22. http://interactivepython.org/courselib/static/pythonds/SortSearch/TheMergeSort.html\n",
    "23. https://www.geeksforgeeks.org/merge-sort/\n",
    "24. http://interactivepython.org/courselib/static/pythonds/SortSearch/TheQuickSort.html\n",
    "25. https://www.geeksforgeeks.org/python-program-for-quicksort/\n",
    "26. https://en.wikipedia.org/wiki/Quicksort\n",
    "27. https://hackernoon.com/timsort-the-fastest-sorting-algorithm-youve-never-heard-of-36b28417f399 \n",
    "28. https://en.wikipedia.org/wiki/Timsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
